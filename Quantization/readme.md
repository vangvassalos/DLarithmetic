# Deep Learning Arithmetic and Numerical Representations
Research material on arithmetic formats, schemes, etc., focused on Convolutional Neural Networks
## Quantization Schemes
- **2015_Chen_ICML**			: Compressing Neural Networks with the Hashing Trick
- **2015_Courbariaux_ICML**	    : BinaryConnect: Training Deep Neural Networks with binary weights during propagations
- **2016_Courbariaux_Arxiv** 	: Binarized Neural Networks: Training NNs with Weights and Activations Constrained to +1 or âˆ’1
- **2016_Gysel_ICLR**			: Hardware-oriented Approximation of Convolutional Neural Networks
- **2016_Li_NIPS**			    : Ternary Weight Networks
- **2016_Lin_Arxiv2**			: Fixed Point Quantization of Deep Convolutional Networks
- **2016_Lin_Arxiv**			: Memory Efficient Nonuniform Quantization for Deep Convolutional Neural Network
- **2016_Rastegari_ECCV** 	    : XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks
- **2016_Zhou_Arxiv**			: DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients
- **2017_Cai_CVPR**			    : Deep Learning with Low Precision by Half-wave Gaussian Quantization
- **2017_Lin_NIPS**			    : Towards Accurate Binary Convolutional Neural Network
- **2017_Meng_Arxiv**			: Two-Bit Networks for Deep Learning on Resource-Constrained Embedded Devices
- **2017_Shin_ICASSP** 		    : Fixed-point optimization of deep neural networks with adaptive step size retraining
- **2017_Zhu_ICLR**			    : Trained Ternary Quantization
- **2017_Zhuang_CVPR**		    : Towards Effective Low-bitwidth Convolutional Neural Networks
- **2018_Athar_Arxiv** 		    : An Overview of Datatype Quantization Techniques for Convolutional Neural Networks
- **2018_Beluja_Arxiv**		    : No Multiplication? No Floating Point? No Problem! Training Networks for Efficient Inference
- **2018_Ding_ASPDAC** 		    : Quantized deep neural networks for energy efficient hardware-based inference
- **2018_Guo_Arxiv**		    : A Survey on Methods and Theories of Quantized Neural Networks
- **2018_Gyzel_ToNNLS**		    : [Ristretto](http://lepsucd.com/?page_id=621): A Framework for Empirical Study of Resource-Efficient Inference in CNNs
- **2018_Jacob_CVPR**		    : Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference
- **2018_Raghuraman_WP**	    : Quantizing deep convolutional networks for efficient inference: A whitepaper from Google
- **2018_Wang_VLSI**            : An Energy-Efficient Architecture for Binary Weight Convolutional Neural Networks
- **2018_Yang_ISCAS**		    : Bit Error Tolerance of a CIFAR-10 Binarized Convolutional Neural Network Processor
- **2018_Zhang_ECCV**		    : LQ-Nets: Learned Quantization for Highly Accurate and Compact Deep Neural Networks
- **2018_Zhou_AAAI**            : Adaptive Quantization for Deep Neural Network
