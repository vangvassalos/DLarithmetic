# Deep Learning Arithmetic and Numerical Representations
Research material on arithmetic formats, schemes, etc., focused on Convolutional Neural Networks
## Numerical Representations
- **2017_Gustafson_IJFSI :**   Beating Floating Point at its Own Game: [Posit Arithmetic](https://github.com/libcg/bfp) 
- **2017_Hill_CLR :**        Rethinking Numerical Representations for Deep Neural Networks
- **2017_Koaster_NIPS :** Flexpoint: An Adaptive Numerical Format for Efficient Training of Deep Neural Networks
- **2018_Drumond :** Training DNNs with Hybrid Block Floating Point
- **2018_Johnson :** Rethinking Floating Point fo Deep Learning
- **2018_Popescu_ARITH :** Flexpoint: Predictive Numerics for Deep Learning

## Quantization Schemes
- **2015_Chen_ICML**			: Compressing Neural Networks with the Hashing Trick
- **2015_Courbariaux_ICML**	: BinaryConnect: Training Deep Neural Networks with binary weights during propagations
- **2016_Courbariaux_Arxiv** 	: Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to +1 or âˆ’1
- **2016_Gysel_ICLR**			: Hardware-oriented Approximation of Convolutional Neural Networks
- **2016_Li_NIPS**			: Ternary Weight Networks
- **2016_Lin_Arxiv2**			: Fixed Point Quantization of Deep Convolutional Networks
- **2016_Lin_Arxiv**			: Memory Efficient Nonuniform Quantization for Deep Convolutional Neural Network
- **2016_Rastegari_ECCV** 	: XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks
- **2016_Zhou_Arxiv**			: DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients
- **2017_Cai_CVPR**			: Deep Learning with Low Precision by Half-wave Gaussian Quantization
- **2017_Lin_NIPS**			: Towards Accurate Binary Convolutional Neural Network
- **2017_Meng_Arxiv**			: Two-Bit Networks for Deep Learning on Resource-Constrained Embedded Devices
- **2017_Shin_ICASSP** 		: Fixed-point optimization of deep neural networks with adaptive step size retraining
- **2017_Zhu_ICLR**			: Trained Ternary Quantization
- **2017_Zhuang_CVPR**		: Towards Effective Low-bitwidth Convolutional Neural Networks
- **2018_Athar_Arxiv** 		: An Overview of Datatype Quantization Techniques for Convolutional Neural Networks
- **2018_Beluja_Arxiv**		: No Multiplication? No Floating Point? No Problem! Training Networks for Efficient Inference
- **2018_Ding_ASPDAC** 		: Quantized deep neural networks for energy efficient hardware-based inference
- **2018_Guo_Arxiv**			: A Survey on Methods and Theories of Quantized Neural Networks
- **2018_Gyzel_ToNNLS**		: Ristretto: A Framework for Empirical Study of Resource-Efficient Inference in Convolutional Neural Networks
- **2018_Jacob_CVPR**			: Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference
- **2018_Raghuraman_WP**		: Quantizing deep convolutional networks for efficient inference: A whitepaper from Google
- **2018_Yang_ISCAS**			: Bit Error Tolerance of a CIFAR-10 Binarized Convolutional Neural Network Processor
- **2018_Zhang_ECCV**			: LQ-Nets: Learned Quantization for Highly Accurate and Compact Deep Neural Networks
